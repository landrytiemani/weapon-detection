# Modular Real-Time Weapon Detection Framework

<div align="center">

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![CUDA 11.8+](https://img.shields.io/badge/CUDA-11.8+-76B900.svg)](https://developer.nvidia.com/cuda-toolkit)

**A Lightweight Modular Real-Time Weapon Detection Framework Using Advanced Computer Vision Techniques for Edge Deployment Optimization**

[Overview](#abstract) â€¢
[Architecture](#architecture) â€¢
[Installation](#installation) â€¢
[Quick Start](#quick-start) â€¢
[Results](#results) â€¢
[Citation](#citation)

</div>

---

## Abstract

This repository contains the complete implementation for doctoral dissertation research at **Harrisburg University of Science and Technology**. The framework achieves significant improvements in weapon detection through a novel **two-stage hierarchical detection pipeline** with **ByteTrack temporal tracking** integration.

### Key Achievements

| Metric | Improvement | Description |
|--------|-------------|-------------|
| ğŸ¯ **mAP@0.5** | +46% | Over single-stage baselines |
| ğŸ“‰ **False Positives** | -71% | Through temporal tracking |
| âš¡ **Real-time** | >30 FPS | On edge devices |
| ğŸ”’ **Privacy** | <10% overhead | Selective face blurring |

---

## Key Contributions

| # | Contribution | Research Question |
|---|--------------|-------------------|
| 1 | **Modular Pipeline Architecture** - Two-stage hierarchical detection with configurable components | RQ1 |
| 2 | **Architecture Comparison** - RT-DETR (Transformer) vs YOLOv8-EfficientViT (CNN) | RQ2 |
| 3 | **Temporal Tracking Integration** - ByteTrack for FP reduction and ID consistency | RQ3 |
| 4 | **Privacy-Preserving Mechanisms** - Selective face blurring with minimal accuracy impact | RQ4 |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MODULAR WEAPON DETECTION PIPELINE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  INPUT   â”‚     â”‚      STAGE 1        â”‚     â”‚       STAGE 2          â”‚   â”‚
â”‚  â”‚  FRAME   â”‚â”€â”€â”€â”€â–¶â”‚  Person Detection   â”‚â”€â”€â”€â”€â–¶â”‚   Weapon Detection     â”‚   â”‚
â”‚  â”‚          â”‚     â”‚   + ByteTrack       â”‚     â”‚   (RT-DETR/EffViT)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                            â”‚                   â”‚
â”‚                            â–¼                            â–¼                   â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                   â”‚ Person Crops    â”‚         â”‚ Post-Processing  â”‚         â”‚
â”‚                   â”‚ â€¢ Scale: 2.5Ã—   â”‚         â”‚ â€¢ NMS (local)    â”‚         â”‚
â”‚                   â”‚ â€¢ Square        â”‚         â”‚ â€¢ NMS (global)   â”‚         â”‚
â”‚                   â”‚ â€¢ Overlap filterâ”‚         â”‚ â€¢ Cross-class    â”‚         â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                         â”‚                   â”‚
â”‚                                                         â–¼                   â”‚
â”‚                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                                                â”‚  Privacy Module  â”‚         â”‚
â”‚                                                â”‚  (Face Blurring) â”‚         â”‚
â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                         â”‚                   â”‚
â”‚                                                         â–¼                   â”‚
â”‚                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                                                â”‚     OUTPUT       â”‚         â”‚
â”‚                                                â”‚ Weapon Detectionsâ”‚         â”‚
â”‚                                                â”‚ + Track IDs      â”‚         â”‚
â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Stage 1: Person Detection & Tracking

| Component | Model | GFLOPs | Purpose |
|-----------|-------|--------|---------|
| Detector | YOLOv8n | 8.7 | Real-time person detection |
| Alternative | SSD-MobileNetV2 | 3.4 | Lightweight option |
| Tracker | ByteTrack | ~0.1 | Multi-object tracking |

### Stage 2: Weapon Detection

| Architecture | Type | GFLOPs | mAP@0.5 | Best For |
|-------------|------|--------|---------|----------|
| **RT-DETR** | Transformer | 81.4 | **0.721** | Accuracy-critical |
| **YOLOv8-EfficientViT** | CNN | 6.2 | 0.669 | Edge deployment |

---

## Research Questions

### RQ1: Modular Ablation Study
> How do individual pipeline components contribute to detection accuracy?

- âœ… Crop scale 2.0-2.5Ã— optimal for weapon visibility
- âœ… Overlap filtering reduces duplicates by >50%
- âœ… Test-time augmentation improves mAP by 3-5%

### RQ2: Architecture Comparison
> How does RT-DETR compare to YOLOv8-EfficientViT?

- âœ… RT-DETR achieves 5.2% higher mAP@0.5
- âœ… EfficientViT maintains 93% accuracy at 13Ã— less compute
- âœ… Larger performance gap on knives vs handguns

### RQ3: Temporal Tracking
> How does ByteTrack affect detection quality?

- âœ… Tracking reduces false positives by 71%
- âœ… Frame gap 3-5 maintains â‰¥95% accuracy
- âœ… 17% speed improvement with gap=3

### RQ4: Privacy Preservation
> Can privacy protection be achieved with minimal impact?

- âœ… Selective face blurring adds only 8% latency
- âœ… Privacy maintains mAP within 2%
- âœ… Pixelation 5% faster than Gaussian blur

---

## Installation

### Prerequisites

- **OS**: Ubuntu 20.04+ / Windows 10+
- **Python**: 3.10+
- **GPU**: NVIDIA with 8GB+ VRAM
- **CUDA**: 11.8+

### Quick Install

```bash
# Clone repository
git clone https://github.com/landrytiemani/weapon-detection-pipeline.git
cd weapon-detection-pipeline

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# Windows: venv\Scripts\activate

# Install PyTorch (CUDA 11.8)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install dependencies
pip install -r requirements.txt

# Download weights
bash scripts/download_weights.sh
```

### Verify Installation

```python
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")

from ultralytics import YOLO
print("âœ“ Installation successful!")
```

---

## Quick Start

### Run Inference

```bash
# Single experiment with tracking
python main_perclass.py --config config.yaml
```

### Run Research Experiments

```bash
# RQ1: Ablation Study
python RQ/run_rq1_ablation.py --config config.yaml

# RQ2: Architecture Comparison
python RQ/run_rq2_architecture.py --config config.yaml

# RQ3: Tracking Experiments
python RQ/run_rq3_tracking.py --config config.yaml

# RQ4: Privacy Analysis
python RQ/run_rq4_privacy.py --config config.yaml

# Compute GFLOPs
python RQ/compute_flops.py --results-dir Results/
```

---

## Project Structure

```
weapon-detection-pipeline/
â”‚
â”œâ”€â”€ main_perclass.py             # ğŸš€ Main entry point
â”œâ”€â”€ config.yaml                  # âš™ï¸ Primary configuration
â”œâ”€â”€ requirements.txt             # ğŸ“¦ Dependencies
â”‚
â”œâ”€â”€ stages/                      # Pipeline stages
â”‚   â”œâ”€â”€ stage_2_persondetection.py    # Person detection + tracking
â”‚   â””â”€â”€ stage_3_weapondetection.py    # Weapon detection
â”‚
â”œâ”€â”€ models/                      # Detection models
â”‚   â”œâ”€â”€ person_detectors/
â”‚   â”‚   â”œâ”€â”€ yolov8_tracker.py         # YOLOv8 + ByteTrack
â”‚   â”‚   â””â”€â”€ ssd_mobilenet_bytetrack.py
â”‚   â””â”€â”€ weapon_detectors/
â”‚       â”œâ”€â”€ rt_detr.py                # RT-DETR (Transformer)
â”‚       â””â”€â”€ yolov8_efficientvit.py    # EfficientViT (CNN)
â”‚
â”œâ”€â”€ tracker/                     # ByteTrack implementation
â”‚   â”œâ”€â”€ byte_tracker.py          # Main tracker
â”‚   â”œâ”€â”€ kalman_filter.py         # Motion prediction
â”‚   â”œâ”€â”€ matching.py              # Hungarian matching
â”‚   â””â”€â”€ basetrack.py             # Track base class
â”‚
â”œâ”€â”€ utils/                       # Utilities
â”‚   â”œâ”€â”€ box_utils.py             # Bounding box operations
â”‚   â”œâ”€â”€ evaluation.py            # mAP calculation
â”‚   â”œâ”€â”€ privacy.py               # Face blurring module
â”‚   â”œâ”€â”€ visualization.py         # Debug visualizations
â”‚   â”œâ”€â”€ flops_utils.py           # GFLOPs computation
â”‚   â””â”€â”€ report_utils.py          # Report generation
â”‚
â”œâ”€â”€ vision/                      # SSD MobileNet support
â”‚   â”œâ”€â”€ ssd/                     # SSD architecture
â”‚   â””â”€â”€ nn/                      # Neural network modules
â”‚
â”œâ”€â”€ RQ/                          # Research experiments
â”‚   â”œâ”€â”€ run_rq1_ablation.py
â”‚   â”œâ”€â”€ run_rq2_architecture.py
â”‚   â”œâ”€â”€ run_rq3_tracking.py
â”‚   â”œâ”€â”€ run_rq4_privacy.py
â”‚   â””â”€â”€ compute_flops.py
â”‚
â”œâ”€â”€ configs/                     # Configuration files
â”‚   â””â”€â”€ person_tracker_bytetrack.yaml
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ EXPERIMENTS.md
â”‚   â””â”€â”€ WEIGHTS.md
â”‚
â”œâ”€â”€ results/                     # Experiment results
â”‚   â””â”€â”€ figures/
â”‚
â””â”€â”€ notebooks/                   # Training notebooks
    â””â”€â”€ Train.ipynb
```

---

## Configuration

### Main Configuration (`config.yaml`)

```yaml
pipeline:
  frames_dir: data/test/images
  labels_dir: data/test/labels

stage_2:
  approach: yolov8_tracker
  crop_scale: 2.5
  use_tracker: true
  frame_gap: 1
  yolov8_tracker:
    model_path: weights/person/yolov8n.pt
    confidence_threshold: 0.15

stage_3:
  approach: yolov8_efficientvit  # or: rt_detr
  imgsz: 512
  nms_iou_threshold: 0.45
  names: ["handgun", "knife"]

privacy:
  enabled: true
  scope: "non_targets"
  face_blur:
    method: "pixelate"
```

---

## Results

### Overall Performance

| Configuration | mAP@0.5 | Precision | Recall | F1 | FPS |
|--------------|---------|-----------|--------|-----|-----|
| **RT-DETR + Tracking** | **0.721** | 0.847 | 0.681 | 0.755 | 24.3 |
| EfficientViT + Tracking | 0.669 | 0.812 | 0.634 | 0.712 | 38.7 |
| Baseline (single-stage) | 0.494 | 0.623 | 0.521 | 0.567 | 42.1 |

### Per-Class Performance

| Class | RT-DETR | EfficientViT | Î” |
|-------|---------|--------------|---|
| **Handgun** | 0.784 | 0.752 | +3.2% |
| **Knife** | 0.658 | 0.586 | +7.2% |

### Key Findings

1. **Two-stage approach** â†’ +46% mAP over single-stage
2. **ByteTrack integration** â†’ -71% false positives
3. **RT-DETR vs EfficientViT** â†’ +5.2% mAP but 13Ã— more compute
4. **Privacy protection** â†’ Only 8% latency overhead

---

## Citation

If you use this code, please cite:

```bibtex
@phdthesis{tiemani2026weapon,
  title     = {A Lightweight Modular Real-Time Weapon Detection Framework Using 
               Advanced Computer Vision Techniques for Edge Deployment Optimization},
  author    = {Tiemani, Landry},
  year      = {2026},
  school    = {Harrisburg University of Science and Technology},
  type      = {Ph.D. Dissertation}
}
```

### Related Works

```bibtex
@inproceedings{zhang2022bytetrack,
  title     = {ByteTrack: Multi-Object Tracking by Associating Every Detection Box},
  author    = {Zhang, Yifu and others},
  booktitle = {ECCV},
  year      = {2022}
}

@inproceedings{lv2024rtdetr,
  title     = {DETRs Beat YOLOs on Real-time Object Detection},
  author    = {Lv, Wenyu and others},
  booktitle = {CVPR},
  year      = {2024}
}
```

---

## License

MIT License - see [LICENSE](LICENSE) for details.

---

## Acknowledgments

- [Ultralytics](https://github.com/ultralytics/ultralytics) - YOLOv8 & RT-DETR
- [ByteTrack](https://github.com/ifzhang/ByteTrack) - Multi-object tracking
- [EfficientViT](https://github.com/microsoft/Cream) - Efficient backbone
- Harrisburg University of Science and Technology

---

<div align="center">

**Developed for Ph.D. Dissertation in Data Sciences**

Harrisburg University of Science and Technology

*Landry Tiemani â€¢ Expected 2026*

</div>
